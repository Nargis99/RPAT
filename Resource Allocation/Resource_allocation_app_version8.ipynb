{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyodbc\n",
    "# !pip install flask\n",
    "# !pip install openpyxl\n",
    "# !pip install excel2img\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import os\n",
    "import sys\n",
    "import pyodbc\n",
    "import openpyxl\n",
    "import warnings\n",
    "import pythoncom\n",
    "import shutil\n",
    "import tempfile\n",
    "import time as t\n",
    "import time\n",
    "import datetime\n",
    "import excel2img\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from flask import * \n",
    "import datetime as dt\n",
    "from datetime import datetime, time\n",
    "from datetime import datetime\n",
    "from pandas import Timestamp\n",
    "from fileinput import filename\n",
    "from distutils.log import debug\n",
    "import win32com.client as win32\n",
    "from openpyxl.styles import Font\n",
    "warnings.filterwarnings('ignore')\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment\n",
    "from openpyxl.styles import PatternFill\n",
    "from openpyxl.styles.colors import Color\n",
    "from openpyxl.drawing.image import Image\n",
    "from werkzeug.utils import secure_filename\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get current directory\n",
    "current_directory = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\nasreenn\\\\Documents\\\\CDM Projects\\\\Resource Allocation'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only excel file is allowed\n",
    "ALLOWED_EXTENSIONS = {'xlsx'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To initate the app\n",
    "app = Flask(__name__)\n",
    "# To activate the upload folder\n",
    "app.config['UPLOAD_FOLDER'] = current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route 1: For Index page\n",
    "@app.route('/')  \n",
    "def main():  \n",
    "    return render_template(\"index.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check the file extension\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: To get details like - user-email-id, file\n",
    "@app.route('/success', methods=['GET', 'POST'])\n",
    "def success():\n",
    "    error = None;\n",
    "    if request.method == 'POST':\n",
    "        if 'file' not in request.files:\n",
    "            flash('No file uploaded')\n",
    "        file = request.files['file']\n",
    "        if file.filename == '':\n",
    "            error = \"No file selected\"\n",
    "        if file and not allowed_file(file.filename):\n",
    "            error = \"File extension should be .xlsx\"\n",
    "        else:\n",
    "            email_u = request.form.get(\"email\")\n",
    "            email_u=email_u.lower()\n",
    "            list_of_email=['maxinn@cdmsmith.com',\n",
    "                           'belcherca@cdmsmith.com',\n",
    "                           'milodovskaiai@cdmsmith.com',\n",
    "                           'martinesea@cdmsmith.com',\n",
    "                           'shahbaza@cdmsmith.com',\n",
    "                           'raufk@cdmsmith.com',\n",
    "                           'nejadaa@cdmsmith.com',\n",
    "                           'goycolj@cdmsmith.com',\n",
    "                           'hollandbm@cdmsmith.com',\n",
    "                           'singhap@cdmsmith.com',\n",
    "                           'sutterd@cdmsmith.com',\n",
    "                           'salousfh@cdmsmith.com',\n",
    "                           'dunbarcj@cdmsmith.com',\n",
    "                           'tripathij@cdmsmith.com',\n",
    "                           'patildv@cdmsmith.com',\n",
    "                           'karthikjm@cdmsmith.com',\n",
    "                           'alamurikr@cdmsmith.com',\n",
    "                           'dattas@cdmsmith.com',\n",
    "                           'acharyyas@cdmsmith.com',\n",
    "                           'patilpa@cdmsmith.com',\n",
    "                           'joaninojj@cdmsmith.com',\n",
    "                           'remediosjh@cdmsmith.com',\n",
    "                           'prashantkk@cdmsmith.com',\n",
    "                           'manishas@cdmsmith.com',\n",
    "                           'sahill@cdmsmith.com',\n",
    "                           'rajath@cdmsmith.com',\n",
    "                           'melendezmunozev@cdmsmith.com',\n",
    "                           'niemiecme@cdmsmith.com',\n",
    "                           'mishraa@cdmsmith.com',\n",
    "                           'dauneriyaa@cdmsmith.com',\n",
    "                           'rangarajr@cdmsmith.com',\n",
    "                           'bhatiat@cdmsmith.com',\n",
    "                           'warudkarpa@cdmsmith.com',\n",
    "                           'kumarip@cdmsmith.com',\n",
    "                           'kalariarb@cdmsmith.com',\n",
    "                           'kwonoe@cdmsmith.com',\n",
    "                           'sachinc@cdmsmith.com',\n",
    "                           'patilvv@cdmsmith.com',\n",
    "                           'suhasc@cdmsmith.com',\n",
    "                           'senguptab@cdmsmith.com',\n",
    "                           'dwivedivt@cdmsmith.com',\n",
    "                           'sinhas@cdmsmith.com',\n",
    "                           'abhishekv@cdmsmith.com',\n",
    "                           'dubeya@cdmsmith.com',\n",
    "                           'paulvs@cdmsmith.com',\n",
    "                           'niranjanar@cdmsmith.com',\n",
    "                           'shelarsv@cdmsmith.com' ,\n",
    "                           'bodkhesp@cdmsmith.com',\n",
    "                           'poddara@cdmsmith.com',\n",
    "                           'stevensrt@cdmsmith.com',\n",
    "                           'saumyat@cdmsmith.com',\n",
    "                           'bokadeas@cdmsmith.com',\n",
    "                           'shuklaa@cdmsmith.com',\n",
    "                           'meenakshij@cdmsmith.com',\n",
    "                           'raysonjc@cdmsmith.com',\n",
    "                           'pandeyg@cdmsmith.com',\n",
    "                           'campbellg@cdmsmith.com',\n",
    "                           'dagaa@cdmsmith.com',\n",
    "                           'bhandarip@cdmsmith.com',\n",
    "                           'nasreenn@cdmsmith.com',\n",
    "                           'kumargn@cdmsmith.com'\n",
    "]\n",
    "            if email_u in list_of_email: \n",
    "                global email_f\n",
    "                email_f = email_u.split('@')[0]\n",
    "                filename2=(file.filename).split('.xlsx')[0]+'-'+email_f+'.xlsx'\n",
    "                filename = secure_filename(filename2)\n",
    "                file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "                return redirect(url_for('external', name=filename, email=email_f, u_email=email_u))\n",
    "            else:\n",
    "                error='Email is not correct'\n",
    "        return  render_template('index.html',error=error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# App\n",
    "@app.route('/external/<name>/<email>/<u_email>', methods=['GET', 'POST']) \n",
    "def external(name,email,u_email):\n",
    "    global temp_folder\n",
    "    temp_folder=current_directory+'\\\\reports'\n",
    "    now = t.time()\n",
    "    for f in os.listdir(temp_folder):\n",
    "        f = os.path.join(temp_folder, f)\n",
    "        if os.stat(f).st_mtime < now - 1 * 86400:\n",
    "             if os.path.isfile(f):\n",
    "                os.remove(os.path.join(temp_folder, f))\n",
    "    DB = {'servername': 'AW02PSQLC007',\n",
    "    'database': 'India_Gtsg'}\n",
    "    conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=' + DB['servername'] + ';DATABASE=' + DB['database'] + ';Trusted_Connection=yes')\n",
    "    rft_report = pd.read_sql_query('''SELECT * FROM [India_Gtsg].[dbo].[RFT_P6]''', conn)\n",
    "    ut = pd.read_sql_query('''SELECT * FROM [India_Gtsg].[dbo].[UT]''', conn)\n",
    "    calendar = pd.read_sql_query('''SELECT * FROM [India_Gtsg].[dbo].[calendar]''', conn)\n",
    "#     data = pd.read_excel('{}'.format(name), sheet_name = 'P6')\n",
    "    # Try reading the Excel file with sheet_name 'P6'\n",
    "    try:\n",
    "        data = pd.read_excel('{}'.format(name), sheet_name='P6')\n",
    "    except:\n",
    "    # If the 'P6' sheet is not found, read the 'Sheet1' sheet\n",
    "        data = pd.read_excel('{}'.format(name), sheet_name='Sheet1')\n",
    "    rft_report = rft_report.drop(['Month-Year', 'Utilization_Target_percent'], axis = 1)\n",
    "    \n",
    "    cursor = conn.cursor()    \n",
    "    SQLCommand = (\"INSERT INTO users(email) VALUES (?)\")  \n",
    "    Values = [u_email]   \n",
    "    cursor.execute(SQLCommand,Values)     \n",
    "    conn.commit()\n",
    "\n",
    "    # Popping the column to change the position\n",
    "    cll1 = rft_report.pop('Utilization_Target')\n",
    "    cll2 = rft_report.pop('Employee_code')\n",
    "    cll3 = rft_report.pop('Resource_Name')\n",
    "    cll4 = rft_report.pop('Project Number')\n",
    "\n",
    "    # Placing the pop up column at first\n",
    "    rft_report.insert(2, 'Utilization_Target', cll1)\n",
    "    # Placing the pop up column at first\n",
    "    rft_report.insert(2, 'Employee_code', cll2)\n",
    "    # Placing the pop up column at first\n",
    "    rft_report.insert(2, 'Resource_Name', cll3)\n",
    "    # Placing the pop up column at first\n",
    "    rft_report.insert(2, 'Project Number', cll4)\n",
    "\n",
    "    # To replace dash (-) & NaN with Zero (0)\n",
    "    except_columns = ['Project', 'Resource (Utilization Target %)/Job Code', 'Project Number', 'Resource_Name', 'Employee_code', 'Utilization_Target']\n",
    "    for column in rft_report.columns:\n",
    "        if column not in except_columns:\n",
    "            rft_report[column] = rft_report[column].replace('-', 0)\n",
    "            rft_report[column] = rft_report[column].replace(np.NaN, 0)\n",
    "\n",
    "    # To change the dtypes of columns\n",
    "    except_columns = ['Project', 'Resource (Utilization Target %)/Job Code', 'Project Number', 'Resource_Name', 'Employee_code', 'Utilization_Target']\n",
    "    for column in rft_report.columns:\n",
    "        if column not in except_columns:\n",
    "            rft_report[column] = pd.to_numeric(rft_report[column], errors = 'coerce')\n",
    "    # Remove extra space\n",
    "    rft_report['Employee_code'] = rft_report['Employee_code'].apply(str.strip)\n",
    "\n",
    "    # Dropping unwanted columns\n",
    "    data1 = data.drop(['Activity Name', 'Curve', 'Spreadsheet Field', 'Remaining Units'], axis = 1)\n",
    "\n",
    "    #changing the format of date and adding it in a new column\n",
    "    calendar['Month_Year'] = calendar['GL Period'].apply(lambda x: x.strftime('%b-%y')) \n",
    "    # Extracting Month columns \n",
    "    data2 = data1.iloc[:, 1:]\n",
    "\n",
    "    x = data2.columns[0]\n",
    "    x2 = x.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    x3 = x2.replace(' 00:00:00', '')\n",
    "    # Rename a column using the .rename() method\n",
    "    data2.rename(columns={x2: x3}, inplace=True)\n",
    "\n",
    "    # Fix time frame according to RFT (18 months)\n",
    "\n",
    "    # To get currentatime\n",
    "    current_date = dt.datetime.now()\n",
    "    # current_date = '2023-10-29'\n",
    "    # Select dates less than current time\n",
    "    current_date_less = calendar[calendar['W/E Date'] <= current_date]\n",
    "    # Convert the datatype\n",
    "    current_date_less['W/E Date'] = current_date_less['W/E Date'].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    # Select last date which is just less than current date - start date\n",
    "    start_d = current_date_less.tail(1)\n",
    "\n",
    "    # Store start date in a list\n",
    "    start = start_d['GL Period'].tolist()\n",
    "    start_dt = start[0]\n",
    "    varr = start_dt.strftime('%Y-%m-%d')\n",
    "    start_date = varr.replace(' 00:00:00', '')\n",
    "\n",
    "    # Convert the string to a datetime object\n",
    "    start_date = dt.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "\n",
    "    # Add 18 months using relativedelta\n",
    "    end_date = start_date + relativedelta(months=17)\n",
    "\n",
    "    # Convert the future date back to a string if needed\n",
    "    end_date = end_date.strftime('%Y-%m-%d')\n",
    "\n",
    "    var1 = data2.columns[0]\n",
    "    var2 = var1.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Specify the format of the date string\n",
    "    date_format = '%Y-%m-%d'  # This format corresponds to 'YYYY-MM-DD'\n",
    "\n",
    "    # Convert the string to a datetime.date object\n",
    "    date_obj = datetime.strptime(var2, date_format).date()\n",
    "    end_date = datetime.strptime(end_date, date_format).date()\n",
    "\n",
    "    # Create a time object with the desired time (e.g., midnight)\n",
    "    time_obj = time(0, 0)  # This represents 00:00:00 (midnight)\n",
    "\n",
    "    # Combine the date and time to create a datetime.datetime object\n",
    "    datetime_obj = datetime.combine(date_obj, time_obj)\n",
    "    datetime_start = datetime.combine(start_date, time_obj)\n",
    "    datetime_end = datetime.combine(end_date, time_obj)\n",
    "    if datetime_start != datetime_obj:\n",
    "        if datetime_start < datetime_obj:\n",
    "            # Selecting columns mentioned in P6\n",
    "            calendar = calendar[calendar['W/E Date'].isin(data2.columns)] \n",
    "            calender = calendar[calendar['W/E Date'] >= datetime_obj]\n",
    "            # Reindexing\n",
    "            calender = calender.reset_index(drop = True)\n",
    "            # Select rows within the date range\n",
    "            final_calender = calender[(calender['W/E Date'] >= datetime_obj) & (calender['W/E Date'] <= datetime_end)]\n",
    "            column_list = final_calender['W/E Date'].tolist()\n",
    "            # Convert the list of timestamps to a list of strings\n",
    "            col_list = [timestamp.strftime('%Y-%m-%d') for timestamp in column_list]\n",
    "            data2 = data2[col_list]\n",
    "            #extracting all month-year values\n",
    "            month = final_calender['Month_Year'].values\n",
    "            data5 = data2.set_axis(month, axis = 'columns')\n",
    "        else: \n",
    "            # Selecting columns mentioned in P6\n",
    "            calendar = calendar[calendar['W/E Date'].isin(data2.columns)] \n",
    "            calender = calendar[calendar['GL Period'] >= datetime_start]\n",
    "            # Reindexing\n",
    "            calender = calender.reset_index(drop = True)\n",
    "            # Select rows within the date range\n",
    "            final_calender = calender[(calender['GL Period'] >= datetime_start) & (calender['GL Period'] <= datetime_end)]\n",
    "            column_list = final_calender['W/E Date'].tolist()\n",
    "            data2 = data2[column_list]\n",
    "            #extracting all month-year values\n",
    "            month = final_calender['Month_Year'].values\n",
    "            data5 = data2.set_axis(month, axis = 'columns')\n",
    "    else: \n",
    "        # Selecting columns mentioned in P6\n",
    "        calendar = calendar[calendar['W/E Date'].isin(data2.columns)] \n",
    "        calender = calendar[calendar['W/E Date'] >= datetime_obj]\n",
    "        # Reindexing\n",
    "        calender = calender.reset_index(drop = True)\n",
    "        # Select rows within the date range\n",
    "        final_calender = calender[(calender['W/E Date'] >= datetime_obj) & (calender['W/E Date'] <= datetime_end)]\n",
    "        column_list = final_calènder['W/E Date'].tolist()\n",
    "        data2 = data2[column_list]\n",
    "        #extracting all month-year values\n",
    "        month = final_calender['Month_Year'].values\n",
    "        data5 = data2.set_axis(month, axis = 'columns')\n",
    "    \n",
    "    # Joining df with p_six\n",
    "    data5.insert(loc = 0, column = 'Activity ID', value = data['Activity ID'])\n",
    "\n",
    "    # Extracting rows with 'Employee Number'\n",
    "    code = data5[data5['Activity ID'].str.contains(\"Employee\")]\n",
    "    # Creating new column named 'Resource Name' by replacing Resource name from col 'Activity ID'\n",
    "    code['Employee Number'] = code['Activity ID'].str.replace('Employee Number: ', '')\n",
    "\n",
    "    # Dropping unwanted columns\n",
    "    dataset = code.drop('Activity ID', axis = 1)\n",
    "\n",
    "    # Popping the column to change the position\n",
    "    val1 = dataset.pop('Employee Number')\n",
    "\n",
    "    # Placing the pop up column at first\n",
    "    dataset.insert(0, 'Employee Number', val1)\n",
    "\n",
    "    # Reseting Index\n",
    "    data6 = dataset.reset_index()\n",
    "\n",
    "    # Dropping unwanted columns\n",
    "    data7 = data6.drop(['index'], axis = 1)\n",
    "\n",
    "    # Changing column name '0' ---> 'Project Number'\n",
    "    data7.rename(columns = {'Employee Number': 'Employee_code'}, inplace = True)\n",
    "\n",
    "    # Remove extra space\n",
    "    data7['Employee_code'] = data7['Employee_code'].apply(str.strip)\n",
    "\n",
    "    # Extracting rows with 'WBS'\n",
    "    wbs = data5[data5['Activity ID'].str.contains(\"WBS: \")]\n",
    "\n",
    "    # Extracting numbers from main column\n",
    "    wbs['num'] = wbs['Activity ID'].str.findall(r'[0-9]+')   #output: [285241, 01, 01], want only oth index i.e., project number\n",
    "\n",
    "    # Removing brackets from each row\n",
    "    wbs[\"Project Number\"] = wbs[\"num\"].str.get(0)\n",
    "\n",
    "    # Project Number in p6\n",
    "    global pn_list\n",
    "    pn_list = list(wbs['Project Number'].unique())\n",
    "    # List of unique Resource Name in p6 data\n",
    "    unique_resource = list(data7['Employee_code'].unique())\n",
    "    unique_resource = [i.strip() for i in unique_resource]\n",
    "\n",
    "    if 'No Employee Number' in unique_resource:\n",
    "        unique_resource.remove('No Employee Number')\n",
    "    \n",
    "    # Extracting rows with 'Resource Name'\n",
    "    pattern = 'Resource|Employee'\n",
    "\n",
    "    code = data5[data5['Activity ID'].str.contains(pattern)]\n",
    "\n",
    "    # Creating new column named 'Resource Name' by replacing Resource name from col 'Activity ID'\n",
    "    code['Info'] = code['Activity ID'].str.replace('Employee Number:', '')\n",
    "    code['Info'] = code['Info'].str.replace('Resource Name: ', '')\n",
    "\n",
    "    # Dropping unwanted columns\n",
    "    codeset = code.drop('Activity ID', axis = 1)\n",
    "    codeset = codeset.reset_index()\n",
    "\n",
    "    codeset = codeset.drop(['index'], axis = 1)\n",
    "\n",
    "    ll2 = codeset.pop('Info')\n",
    "\n",
    "    # Placing the pop up column at first\n",
    "    codeset.insert(0, 'Info', ll2)\n",
    "\n",
    "    # Apply strip to the 'names' column\n",
    "    codeset['Info'] = codeset['Info'].str.strip()\n",
    "    \n",
    "    # Extracting rows with 'Resource Name'\n",
    "    pattern = 'Resource'\n",
    "\n",
    "    cod = data5[data5['Activity ID'].str.contains(pattern)]\n",
    "\n",
    "    # Creating new column named 'Resource Name' by replacing Resource name from col 'Activity ID'\n",
    "    cod['Info'] = cod['Activity ID'].str.replace('Resource Name: ', '')\n",
    "\n",
    "    # Dropping unwanted columns\n",
    "    codes = cod.drop('Activity ID', axis = 1)\n",
    "    codes = codes.reset_index()\n",
    "\n",
    "    codes = codes.drop(['index'], axis = 1)\n",
    "\n",
    "    lo2 = codes.pop('Info')\n",
    "\n",
    "    # Placing the pop up column at first\n",
    "    codes.insert(0, 'Info', lo2)\n",
    "\n",
    "    # Apply strip to the 'names' column\n",
    "    codes['Info'] = codes['Info'].str.strip()\n",
    "\n",
    "    # Dictionary to map old column names to new column names\n",
    "    new_columns = {'Info': 'Resource_Name'}\n",
    "\n",
    "    # Rename columns using the 'rename' method\n",
    "    codes = codes.rename(columns=new_columns)\n",
    "\n",
    "    # Select p6 resource from rft for 'Employee code and UT'\n",
    "    rft1 = rft_report[rft_report['Employee_code'].isin(unique_resource)]\n",
    "\n",
    "    # Get unique list of code and ut for resource in p6\n",
    "    rft2 = rft1[['Resource_Name', 'Employee_code', 'Utilization_Target']].drop_duplicates()\n",
    "    rft2 = rft2.reset_index()\n",
    "\n",
    "    # Dropping unwanted columns\n",
    "    rft3 = rft2.drop(['index'], axis = 1)\n",
    "\n",
    "    # Merging data7 & rft3 for 'Employee_code' & 'Utilization_Target'\n",
    "    merged_df = pd.merge(data7, rft3, on = 'Employee_code', how = 'right')\n",
    "    # Popping the column to change the position\n",
    "    vll1 = merged_df.pop('Utilization_Target')\n",
    "    vll2 = merged_df.pop('Resource_Name')\n",
    "\n",
    "    # Placing the pop up column at first\n",
    "    merged_df.insert(1, 'Utilization_Target', vll1)\n",
    "    # Placing the pop up column at first\n",
    "    merged_df.insert(0, 'Resource_Name', vll2)\n",
    "\n",
    "    # To replace dash (-) & NaN with Zero (0)\n",
    "    except_col = ['Resource_Name', 'Employee_code', 'Utilization_Target Name']\n",
    "    for column in merged_df.columns:\n",
    "        if column not in except_col:\n",
    "            #data9[column] = data9[column].replace('-', 0)\n",
    "            merged_df[column] = merged_df[column].replace(np.NaN, 0)\n",
    "\n",
    "    # Convert data types for columns except the excluded ones\n",
    "    exclude_columns = ['Resource_Name', 'Employee_code', 'Utilization_Target Name']\n",
    "    for column in merged_df.columns:\n",
    "        if column not in exclude_columns:\n",
    "            merged_df[column] = pd.to_numeric(merged_df[column], errors='coerce')  # You can use other conversion functions if needed\n",
    "    # Popping the column to change the position\n",
    "    cal1 = ut.pop('Utilization_Target')\n",
    "    cal2 = ut.pop('Employee_code')\n",
    "    cal3 = ut.pop('Resource_Name')\n",
    "\n",
    "    # Placing the pop up column at first\n",
    "    ut.insert(1, 'Utilization_Target', cal1)\n",
    "    # Placing the pop up column at first\n",
    "    ut.insert(1, 'Employee_code', cal2)\n",
    "    # Placing the pop up column at first\n",
    "    ut.insert(1, 'Resource_Name', cal3)\n",
    "    ut = ut.drop('Utilization_Target_percent', axis = 1)\n",
    "\n",
    "    ## Select specific uts\n",
    "    sel = ut[ut['Employee_code'].isin(unique_resource)]\n",
    "\n",
    "    # Ut with only combine columns\n",
    "    ut_com = sel[merged_df.columns]\n",
    "\n",
    "    # RESET INDEX\n",
    "    ut_com = ut_com.reset_index(drop = True)\n",
    "\n",
    "    # Groupby RFT_REPORT based on 'Resource Name' & 'Project Number'\n",
    "    grouped = rft_report.groupby(['Resource_Name', 'Employee_code', 'Utilization_Target','Project Number']).sum()\n",
    "\n",
    "    # Redexing to apply filter on 'Resource Name'\n",
    "    grouped_rft = grouped.reset_index()\n",
    "\n",
    "    # P6 in RFT\n",
    "    set1 = grouped_rft[grouped_rft['Employee_code'].isin(unique_resource)]\n",
    "\n",
    "    # Selecting rft based on Project number not in final_p6 (NOT EXIST/EXIST CASE)\n",
    "    set2 = set1[~set1['Project Number'].isin(pn_list)]\n",
    "\n",
    "    # set2 with only merged_df columns ---index issue\n",
    "    set2 = set2[merged_df.columns]\n",
    "\n",
    "    # Groupby RFT_REPORT based on 'Resource Name' & 'Project Number'\n",
    "    set3 = set2.groupby(['Resource_Name', 'Employee_code', 'Utilization_Target']).sum()\n",
    "    set3 = set3.reset_index()\n",
    "    ## 1. Conbine data9 and set3\n",
    "    combine = merged_df.set_index(['Resource_Name','Employee_code','Utilization_Target']).add(set3.set_index(['Resource_Name','Employee_code','Utilization_Target']), fill_value = 0.0).reset_index()\n",
    "\n",
    "    # List of unique Resource Name in p6 data\n",
    "    listrft = list(combine['Employee_code'].unique())\n",
    "    listrft = [i.strip() for i in listrft]\n",
    "    combine1 = ut_com.set_index(['Resource_Name','Employee_code','Utilization_Target']).subtract(combine.set_index(['Resource_Name','Employee_code','Utilization_Target']), fill_value=0).reset_index()\n",
    "\n",
    "    # List of missing resource name (not in rft)\n",
    "    resource = list(combine1['Employee_code'].unique())\n",
    "    resource = [i.strip() for i in resource]\n",
    "\n",
    "    # Not in rft\n",
    "    missing_resource = set(unique_resource) - set(resource)\n",
    "    miss_df = data7[data7['Employee_code'].isin(missing_resource)]\n",
    "    \n",
    "    match = {'Resource_Name': [],\n",
    "         'Employee_code': [] }\n",
    "\n",
    "    for idx, i in enumerate(missing_resource):\n",
    "        # Use isin() to find the matching row index\n",
    "        matching_index = codeset[codeset['Info'].isin([i])].index\n",
    "\n",
    "        try:\n",
    "            # Extract the row immediately following the matching index\n",
    "            if not matching_index.empty:\n",
    "                following_row_index = matching_index[0] - 1      # code row\n",
    "                following_row = codeset.iloc[following_row_index]   # code numer\n",
    "\n",
    "                match['Employee_code'].append(codeset.iloc[matching_index[0]]['Info'])\n",
    "                match['Resource_Name'].append(following_row['Info'])\n",
    "            else:\n",
    "                continue\n",
    "        except IndexError:\n",
    "            match['Employee_code'].append(i)\n",
    "            match['Resource_Name'].append('-')\n",
    "\n",
    "    df = pd.DataFrame(match)\n",
    "    def concat(df1, df2):\n",
    "        try:\n",
    "            if df1.empty and df2.empty:\n",
    "                return df2\n",
    "            else:\n",
    "                # Merge DataFrames based on the 'ID' column\n",
    "                missdf = pd.merge(df1, df2, on='Employee_code', how='inner')\n",
    "                return missdf\n",
    "        except:\n",
    "            return df2\n",
    "    missdf = concat(df, miss_df)\n",
    "    def con(df1, df2):\n",
    "        try:\n",
    "            if df2.empty:\n",
    "                return df1\n",
    "            else:\n",
    "                # Merging data7 & rft3 for 'Employee_code' & 'Utilization_Target'\n",
    "                combb = pd.concat([df1, df2], axis = 0)\n",
    "                return combb\n",
    "        except AttributeError:\n",
    "            if df2.empty:\n",
    "                return df1\n",
    "\n",
    "    combb = con(combine1, missdf)\n",
    "\n",
    "    # To replace dash (-) & NaN with Zero (0)\n",
    "    incl_col = ['Resource_Name', 'Employee_code', 'Utilization_Target Name']\n",
    "    for column in combb.columns:\n",
    "        if column in incl_col:\n",
    "            #data9[column] = data9[column].replace('-', 0)\n",
    "            combb[column] = combb[column].replace(np.NaN, '')\n",
    "\n",
    "    combb['Utilization_Target'] = combb['Utilization_Target'].astype(str)\n",
    "    combb['Employee_code'] = combb['Employee_code'].astype(str)\n",
    "    \n",
    "    result = {'Resource_Name': [],\n",
    "         'Employee_code': [] }\n",
    "\n",
    "    # Use isin() to find the matching row index\n",
    "    matching_index = codeset[codeset['Info']== 'No Employee Number'].index\n",
    "    for idx, i in enumerate(matching_index):\n",
    "        try:\n",
    "            # Extract the row immediately following the matching index\n",
    "            if not matching_index.empty:\n",
    "                following_row_index = i - 1      # code row\n",
    "                following_row = codeset.iloc[following_row_index]   # code numer\n",
    "\n",
    "                result['Employee_code'].append('-')\n",
    "                result['Resource_Name'].append(following_row['Info'])\n",
    "            else:\n",
    "                continue\n",
    "        except IndexError:\n",
    "            result['Employee_code'].append(i)\n",
    "            result['Resource_Name'].append('-')\n",
    "\n",
    "    dff = pd.DataFrame(result)\n",
    "\n",
    "    def contt(df1, df2):\n",
    "        try:\n",
    "            if df2.empty:\n",
    "                return df2\n",
    "            else:\n",
    "                # Merging data7 & rft3 for 'Employee_code' & 'Utilization_Target'\n",
    "                mrg = pd.merge(df1, df2, on='Resource_Name', how='inner')\n",
    "                return mrg\n",
    "        except AttributeError:\n",
    "            if df2.empty:\n",
    "                return df2\n",
    "\n",
    "    mrg1 = contt(codes, dff)  \n",
    "    \n",
    "    def cont(df1, df2):\n",
    "        try:\n",
    "            if df1.empty:\n",
    "                return df2\n",
    "            elif df1.empty and df2.empty:\n",
    "                return df1\n",
    "            elif df2.empty:\n",
    "                return df1\n",
    "            else:\n",
    "                # Merging data7 & rft3 for 'Employee_code' & 'Utilization_Target'\n",
    "                mrg = pd.concat([df1, df2], axis = 0)\n",
    "                return mrg\n",
    "        except AttributeError:\n",
    "            if df1.empty:\n",
    "                return df2\n",
    "\n",
    "    mrg = cont(missdf, mrg1)\n",
    "\n",
    "    # # Merging data7 & rft3 for 'Employee_code' & 'Utilization_Target'\n",
    "    # mrg = pd.concat([missdf, result], axis = 0)\n",
    "#     def asterik(df):\n",
    "#         try:\n",
    "#             if df.empty :\n",
    "#                 return df\n",
    "#             else:\n",
    "#                 # # Add '*' after each value in columns 'A' and 'B'\n",
    "#                 df['Resource_Name'] = df['Resource_Name'].astype(str) + '*'\n",
    "#                 return mrg\n",
    "#         except AttributeError:\n",
    "#             if df.empty:\n",
    "#                 return df\n",
    "\n",
    "#     mrg = asterik(mrg)\n",
    "    \n",
    "    def vrt(df1, df2):\n",
    "        try:\n",
    "            if df2.empty:\n",
    "                return df1\n",
    "            else:\n",
    "                # Merging data7 & rft3 for 'Employee_code' & 'Utilization_Target'\n",
    "                comb = pd.concat([df1, df2], axis = 0)\n",
    "                comb['Utilization_Target'] = comb['Utilization_Target'].astype(str)\n",
    "                comb['Employee_code'] = comb['Employee_code'].astype(str)\n",
    "                return comb\n",
    "        except AttributeError:\n",
    "            if df2.empty:\n",
    "                return df1\n",
    "\n",
    "    comb = vrt(combine1, mrg)\n",
    "\n",
    "    # To replace dash (-) & NaN with Zero (0)\n",
    "    # To replace dash (-) & NaN with Zero (0)\n",
    "    except_col = ['Employee_code', 'Utilization_Target']\n",
    "    for column in comb.columns:\n",
    "        if column in except_col:\n",
    "            #data9[column] = data9[column].replace('-', 0)\n",
    "            comb[column] = comb[column].replace('nan', '-')\n",
    "    comb = comb.round(0)\n",
    "    comb = comb.reset_index()\n",
    "    comb = comb.drop('index', axis=1)\n",
    "\n",
    "    def highlight_max(cell_value):\n",
    "        highlight = 'background-color: tomato;'\n",
    "        color = 'background-color: lightblue;'\n",
    "        default = ''\n",
    "        if type(cell_value) in [float]:\n",
    "            if cell_value < 0:\n",
    "                return highlight\n",
    "            else:\n",
    "                return color\n",
    "        else:\n",
    "            if '*' in cell_value:\n",
    "                return 'color:red'\n",
    "        return default\n",
    "\n",
    "    # # Apply the custom styling to the entire DataFrame\n",
    "    # # Apply color to combine\n",
    "    final_data = comb.style.applymap(highlight_max).format(precision = 0)\n",
    "    test = comb.copy()\n",
    "\n",
    "    # AVAILABLE RESOURCES\n",
    "    test = test[(test[test.columns[3:]] >= 0).all(axis=1)]\n",
    "    pos_resources = test['Employee_code'].unique()\n",
    "\n",
    "    # Selecting pos_resources from arr\n",
    "    neg_info = combine1[~combine1['Employee_code'].isin(pos_resources)]\n",
    "    neg_info = neg_info.reset_index(drop=True)\n",
    "\n",
    "    # Calling from rft: set3\n",
    "    rft_set4 = set3[set3['Employee_code'].isin(combine1['Employee_code'].unique())].reset_index(drop=True)\n",
    "\n",
    "    # Calling from p6: data9\n",
    "    data10 = merged_df[merged_df['Employee_code'].isin(combine1['Employee_code'].unique())].reset_index(drop=True)\n",
    "    data10 = data10.sort_values(by = 'Employee_code')\n",
    "    # Calling from ut: ut_com\n",
    "    ut1 = ut_com[ut_com['Employee_code'].isin(combine1['Employee_code'].unique())].reset_index(drop=True)\n",
    "\n",
    "    ## 1. Combine data10 and rft_set4\n",
    "    av_hours = ut1.set_index(['Resource_Name','Employee_code','Utilization_Target']).subtract(rft_set4.set_index(['Resource_Name','Employee_code','Utilization_Target']), fill_value=0).reset_index()\n",
    "    av_hours = av_hours.sort_values(by = 'Employee_code')\n",
    "    \n",
    "    \n",
    "    mssdf = missdf.copy()\n",
    "    mssdf['Utilization_Target'] = 0\n",
    "    \n",
    "    # Popping the column to change the position\n",
    "    mdf = mssdf.pop('Utilization_Target')\n",
    "\n",
    "    # Placing the pop up column at first\n",
    "    mssdf.insert(2, 'Utilization_Target', mdf)\n",
    "    \n",
    "    data11 = pd.concat([data10,mssdf], axis=0, ignore_index=True)\n",
    "    \n",
    "    # Change all values to 0 in all columns except 'ut' and 'employee'\n",
    "    mssdf.loc[:, ~mssdf.columns.isin(['Resource_Name','Employee_code','Utilization_Target'])] = 0\n",
    "    \n",
    "    av_hours1 = pd.concat([mssdf, av_hours], axis=0, ignore_index=True)\n",
    "    av_hours1=av_hours1.sort_values('Employee_code')\n",
    "    data11=data11.sort_values('Employee_code')\n",
    "    \n",
    "    # Compare av_hours & data10\n",
    "    compa = data11.set_index(['Resource_Name','Employee_code','Utilization_Target']).compare(av_hours1.set_index(['Resource_Name','Employee_code','Utilization_Target']), align_axis = 1).rename(columns={'self': 'P6 Hrs', 'other': 'Avail Hrs'}, level=1).reset_index()\n",
    "\n",
    "    # Replace Nan with 0\n",
    "    comp = compa.replace(np.NaN, 0)\n",
    "    comp=comp.round(0)\n",
    "\n",
    "    Columns_to_exclude = ['Resource_Name','Employee_code','Utilization_Target']\n",
    "    subset = data11.loc[:, ~data11.columns.isin(Columns_to_exclude)].sum(axis=1)\n",
    "    data11['Total P6 Hours'] = subset\n",
    "    data12 = data11[['Employee_code','Total P6 Hours']]\n",
    "    av_hours2=pd.merge(av_hours1,data12, how='left')\n",
    "    # Compare av_hours & data10\n",
    "    com = data11.set_index(['Resource_Name','Employee_code','Utilization_Target','Total P6 Hours']).compare(av_hours2.set_index(['Resource_Name','Employee_code','Utilization_Target','Total P6 Hours']), align_axis = 1).rename(columns={'self': 'P6 Hrs', 'other': 'Avail Hrs'}, level=1).reset_index()\n",
    "\n",
    "    # Replace Nan with 0\n",
    "    co = com.replace(np.NaN, 0)\n",
    "    co=co.round(0)\n",
    "    \n",
    "    co = co.sort_values('Total P6 Hours', ascending=False)\n",
    "    missed_emp_list = miss_df['Employee_code'].to_list()\n",
    "\n",
    "    co['Utilization_Target'] = co['Utilization_Target'].astype(object)\n",
    "    co['Utilization_Target'] = co['Utilization_Target'].astype(str) + '%'\n",
    "    co.rename(columns = {'Utilization_Target':'UT(%)'}, inplace = True)\n",
    "    co.rename(columns = {'Resource_Name':'Resource Name'}, inplace = True)\n",
    "    co.rename(columns = {'Employee_code':'Employee No.'}, inplace = True)\n",
    "    \n",
    "    compare  = co.copy()\n",
    "    \n",
    "    # Coloring\n",
    "    def missed_emp(co, missed_emp):\n",
    "        mask = co['Employee No.'].isin(missed_emp)\n",
    "        co_style = co.style.apply(lambda x: ['color: red' if v else '' for v in mask], subset=['Resource Name', 'Employee No.'])\n",
    "        return co_style\n",
    "\n",
    "    def compare_and_color(row, compare, missed_emp_list):\n",
    "        styles = [''] * len(row)\n",
    "        for i in range(len(compare.columns)-1):\n",
    "            col1 = compare.columns[i]\n",
    "            col2 = compare.columns[i + 1]\n",
    "            if col1[0] == col2[0]:\n",
    "                if row[col2] - row[col1]> -40:\n",
    "                    styles[compare.columns.get_loc(col2)] = 'background-color: lightblue; color: black'\n",
    "                elif -40 > row[col2] - row[col1] > -60:\n",
    "                    styles[compare.columns.get_loc(col2)] = 'background-color: yellow; color: black'\n",
    "                elif -60 > row[col2] - row[col1] > -80:\n",
    "                    styles[compare.columns.get_loc(col2)] = 'background-color: orange; color: black'\n",
    "                else:\n",
    "                    styles[compare.columns.get_loc(col2)] = 'background-color: red; color: black'\n",
    "        return styles\n",
    "\n",
    "    # Apply missed_emp function\n",
    "    styled_co_missed_emp = missed_emp(co, missed_emp_list)\n",
    "\n",
    "    # Apply compare_and_color function\n",
    "    styled_co = styled_co_missed_emp.apply(compare_and_color, compare=co, missed_emp_list=missed_emp_list, axis=1).format(precision=1)\n",
    "    \n",
    "    file_name = temp_folder+\"/Resource Analysis Report-{}-{}.xlsx\".format(email,pn_list[0])\n",
    "    # Saving two dataframes 'final_data' & 'styled_df' in excel file with defined sheet name\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "    #     final_data.to_excel(writer, sheet_name = 'Summary',index = False)\n",
    "        styled_co.to_excel(writer, sheet_name = 'Summary')\n",
    "\n",
    "    # Opening the excel file\n",
    "    workbook = openpyxl.load_workbook(file_name)\n",
    "    # worksheet1 = workbook['Summary']\n",
    "    worksheet2 = workbook[\"Summary\"]\n",
    "    list1 = data7[~data7['Employee_code'].isin(listrft)]\n",
    "    list2 = list(list1['Employee_code'].unique())\n",
    "    list3 = [i.strip() for i in list2]\n",
    "\n",
    "    column_widths = {'A': 16, 'B': 16, 'C': 14, 'D':10, 'E': 16}\n",
    "    for column, width in column_widths.items():\n",
    "        try:\n",
    "            if column in ['A', 'B', 'C', 'D', 'E']:\n",
    "                worksheet2.column_dimensions[column].width = width\n",
    "        except:\n",
    "            for column in worksheet2.columns:\n",
    "                adjusted_width = 25\n",
    "                worksheet2.column_dimensions[column[0].column_letter].width = adjusted_width\n",
    "\n",
    "    # Text alignment\n",
    "    max_row = worksheet2.max_row\n",
    "    max_column = worksheet2.max_column\n",
    "    for row in range(1, max_row + 1):\n",
    "        for column in range(1, max_column + 1):\n",
    "            if column != 2:\n",
    "                cell = worksheet2.cell(row = row, column = column)\n",
    "                alignment = Alignment(horizontal = 'center')\n",
    "                cell.alignment = alignment\n",
    "            else:\n",
    "                pass\n",
    "    new_serial_numbers = list(range(1,len(compare['Resource Name'])+1))  # Replace this with your desired new serial numbers\n",
    "\n",
    "    # Iterate through the rows and update the serial numbers in column A\n",
    "    for i, new_value in enumerate(new_serial_numbers):\n",
    "        row_number = i + 4  # Rows are 1-indexed in Excel\n",
    "        worksheet2.cell(row = row_number, column=1, value=new_value)\n",
    "    worksheet2.column_dimensions['A'].hidden = True\n",
    "    worksheet2.delete_rows(idx=3)   #for rows\n",
    "\n",
    "    # Inserting image\n",
    "    img = Image('image.png')\n",
    "    img.width = 450\n",
    "    img.height = 200\n",
    "    col1 = \"A\"\n",
    "    row_num1 = str(len(compare['Resource Name']) + 5)\n",
    "    row_call1 = col1 + row_num1\n",
    "    img.anchor = row_call1\n",
    "    worksheet2.add_image(img)\n",
    "\n",
    "    # Coloring the headers and index\n",
    "    col_color = PatternFill(patternType='solid', fgColor = Color('D8E4BC') )\n",
    "    #row_color = PatternFill(patternType='solid',fgColor=Color ('E0EEEE') )\n",
    "    for i in range(1,len(compare.columns) + 2):\n",
    "        worksheet2.cell(row = 1, column = i).fill = col_color\n",
    "    workbook.save(file_name)\n",
    "    workbook.close()\n",
    "\n",
    "    l1=pn_list[0]+'-'+email\n",
    "    # l2=pn_list[0]+'-'+u_email\n",
    "    l3 = 'No_OA'\n",
    "    excel2img.export_img(file_name, \"static/{}.png\".format(l1), \"Summary\", None)\n",
    "    # excel2img.export_img(file_name, \"static/{}.png\".format(l2), \"Overallocated Resources\", None)\n",
    "    excel2img.export_img('No_OA.xlsx', \"static/{}.png\".format(l3), \"Sheet1\", \"A1:B1\")\n",
    "    if compare.empty:\n",
    "        return redirect(url_for('temp_fold', f2=name, temp_folder=temp_folder,l1=l2))\n",
    "    else:\n",
    "        return redirect(url_for('temp_fold', f2=name, temp_folder=temp_folder,l1=l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/temp_fold/<f2>/<temp_folder>/<l1>') \n",
    "def temp_fold(f2,temp_folder,l1):\n",
    "    tf=temp_folder\n",
    "    src_path2 = os.getcwd()+\"/\"+f2\n",
    "    dst_path2 = temp_folder+\"/\"+f2\n",
    "    shutil.move(src_path2, dst_path2)\n",
    "    #return redirect('/gallery')\n",
    "    return redirect(url_for('gallery',l1=l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/gallery/<l1>') \n",
    "def gallery(l1):\n",
    "    return render_template('gallery.html',l1=l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ownload_file() func. will download 'Resource Analysis Report' excel in internal system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/download', methods=['GET'])\n",
    "def download_file():\n",
    "    return send_file(temp_folder+\"/Resource Analysis Report-{}-{}.xlsx\".format(email_f,pn_list[0]), as_attachment = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ack() function will transfer the control back to 'index.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/go_back', methods = ['GET'])\n",
    "def back():\n",
    "    return render_template('index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:443\n",
      " * Running on http://192.168.1.110:443\n",
      "Press CTRL+C to quit\n",
      "192.168.1.110 - - [26/Mar/2024 20:59:14] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.1.110 - - [26/Mar/2024 20:59:14] \"GET /static/logo.png HTTP/1.1\" 200 -\n",
      "192.168.1.110 - - [26/Mar/2024 20:59:15] \"GET /static/logo.png HTTP/1.1\" 304 -\n",
      "192.168.1.110 - - [26/Mar/2024 20:59:25] \"POST /success HTTP/1.1\" 302 -\n",
      "192.168.1.110 - - [26/Mar/2024 21:01:35] \"GET /external/Book9-nasreenn.xlsx/nasreenn/nasreenn%40cdmsmith.com HTTP/1.1\" 500 -\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\nasreenn\\AppData\\Local\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2548, in __call__\n",
      "    return self.wsgi_app(environ, start_response)\n",
      "  File \"C:\\Users\\nasreenn\\AppData\\Local\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2528, in wsgi_app\n",
      "    response = self.handle_exception(e)\n",
      "  File \"C:\\Users\\nasreenn\\AppData\\Local\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 2525, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\nasreenn\\AppData\\Local\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1822, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\nasreenn\\AppData\\Local\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1820, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\nasreenn\\AppData\\Local\\anaconda3\\lib\\site-packages\\flask\\app.py\", line 1796, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"C:\\Users\\nasreenn\\AppData\\Local\\Temp\\ipykernel_21368\\4153121361.py\", line 14, in external\n",
      "    conn = pyodbc.connect('DRIVER={SQL Server}; SERVER=' + DB['servername'] + ';DATABASE=' + DB['database'] + ';Trusted_Connection=yes')\n",
      "pyodbc.OperationalError: ('08001', '[08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]SQL Server does not exist or access denied. (17) (SQLDriverConnect); [08001] [Microsoft][ODBC SQL Server Driver][DBNETLIB]ConnectionOpen (Connect()). (53)')\n",
      "192.168.1.110 - - [26/Mar/2024 21:01:35] \"GET /external/Book9-nasreenn.xlsx/nasreenn/nasreenn%40cdmsmith.com?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1\" 200 -\n",
      "192.168.1.110 - - [26/Mar/2024 21:01:35] \"GET /external/Book9-nasreenn.xlsx/nasreenn/nasreenn%40cdmsmith.com?__debugger__=yes&cmd=resource&f=debugger.js HTTP/1.1\" 200 -\n",
      "192.168.1.110 - - [26/Mar/2024 21:01:35] \"GET /external/Book9-nasreenn.xlsx/nasreenn/nasreenn%40cdmsmith.com?__debugger__=yes&cmd=resource&f=console.png HTTP/1.1\" 200 -\n",
      "192.168.1.110 - - [26/Mar/2024 21:01:35] \"GET /external/Book9-nasreenn.xlsx/nasreenn/nasreenn%40cdmsmith.com?__debugger__=yes&cmd=resource&f=console.png HTTP/1.1\" 304 -\n",
      "192.168.1.110 - - [26/Mar/2024 21:31:16] \"GET / HTTP/1.1\" 200 -\n",
      "192.168.1.110 - - [26/Mar/2024 21:31:16] \"GET /static/logo.png HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "app.run(host = \"0.0.0.0\",port=443, debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
